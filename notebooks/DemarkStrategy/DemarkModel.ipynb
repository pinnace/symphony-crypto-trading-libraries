{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import turicreate as tc\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = \"../../symphony/backtest/results/demark/demark.csv.gz\"\n",
    "data_columns = [\n",
    "    \"IsPerfect\", \n",
    "    \"DWaveUp\", \n",
    "    \"DWaveDown\", \n",
    "    \"DerivativeOscillator\", \n",
    "    \"DerivativeOscillatorSignal\", \n",
    "    \"DerivativeOscillatorRule\", \n",
    "    \"CandlestickPattern\",\n",
    "    \"CandlestickPatternDirection\",\n",
    "    \"MassIndex\",\n",
    "    \"NATR\",\n",
    "    \"ADX\",\n",
    "    \"TDREI\",\n",
    "    \"TDPOQ\",\n",
    "    \"DemarkerI\",\n",
    "    \"DemarkerIOversold\",\n",
    "    \"DemarkerIOverbought\",\n",
    "    \"TDPressure\",\n",
    "    \"TDPressureOversold\",\n",
    "    \"TDPressureOverbought\",\n",
    "    \"IsZigZag\",\n",
    "    \"HarmonicsPattern\"\n",
    "]\n",
    "data_columns = ['IsPerfect', 'dwave_up', 'dwave_down', 'derivative_oscillator', 'derivative_oscillator_signal', 'DerivativeOscillatorRule', 'candlestick_pattern', 'candlestick_pattern_direction', 'mass_index', 'natr', 'adx', 'td_rei', 'td_poq', 'td_demarker_i', 'DemarkerIOversold', 'DemarkerIOverbought', 'td_pressure', 'TDPressureOversold', 'TDPressureOverbought', 'zigzag', 'harmonic']\n",
    "numeric_features = [\n",
    "    \"DerivativeOscillator\",\n",
    "    \"DerivativeOscillatorSignal\", \n",
    "    \"MassIndex\",\n",
    "    \"TDREI\",\n",
    "    \"DemarkerI\",\n",
    "    \"TDPressure\",\n",
    "]\n",
    "categorical_features = [\n",
    "    \"IsPerfect\", \n",
    "    \"DWaveUp\", \n",
    "    \"DWaveDown\",\n",
    "    \"DerivativeOscillatorRule\",\n",
    "    \"CandlestickPattern\",\n",
    "    \"CandlestickPatternDirection\",\n",
    "    \"TDPOQ\",\n",
    "    \"DemarkerIOversold\",\n",
    "    \"DemarkerIOverbought\",\n",
    "    \"TDPressureOversold\",\n",
    "    \"TDPressureOverbought\",\n",
    "    \"IsZigZag\",\n",
    "    \"HarmonicsPattern\"\n",
    "]\n",
    "label_column = [\n",
    "    \"Profitable\"\n",
    "]\n",
    "results_df = pd.read_csv(location, compression='gzip')\n",
    "\n",
    "def apply_transformations(sf):\n",
    "    for row in sf:\n",
    "        if row[\"Profitable\"] == \"True\" and row[\"PNLPerc\"] <0:\n",
    "            row[\"Profitable\"] = \"False\"\n",
    "    \n",
    "    sf[\"Profitable\"] = sf[\"Profitable\"].apply(lambda x: 1 if x == \"True\" else 0)\n",
    "    sf = sf[data_columns + label_column]\n",
    "    sf['dwave_up'] = sf['dwave_up'].astype(str)\n",
    "    sf['dwave_down'] = sf['dwave_down'].astype(str)\n",
    "    \n",
    "    if 'harmonic' in sf.column_names():\n",
    "        sf['harmonic'] = sf['harmonic'].astype(str)\n",
    "    if 'td_pressure' in sf.column_names():\n",
    "        sf = sf.dropna('td_pressure')\n",
    "    return sf\n",
    "\n",
    "def get_data(results_df):\n",
    "    return tc.SFrame(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /root/trading-libs/symphony/backtest/results/demark/demark.csv.gz</pre>"
      ],
      "text/plain": [
       "Finished parsing file /root/trading-libs/symphony/backtest/results/demark/demark.csv.gz"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.102469 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.102469 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[float,float,float,float,str,int,str,str,str,str,int,int,float,float,str,str,str,float,float,float,float,float,str,float,str,str,float,str,str,str,str,str,float,float,int,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /root/trading-libs/symphony/backtest/results/demark/demark.csv.gz</pre>"
      ],
      "text/plain": [
       "Finished parsing file /root/trading-libs/symphony/backtest/results/demark/demark.csv.gz"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 13475 lines in 0.089509 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 13475 lines in 0.089509 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = get_data(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(results_df[results_df[\"Symbol\"] == \"AIONUSDT\"][\"Profitable\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ONTUSDT',\n",
       " 'XMRUSDT',\n",
       " 'EOSUSDT',\n",
       " 'BTTUSDT',\n",
       " 'ETCUSDT',\n",
       " 'ATOMUSDT',\n",
       " 'HOTUSDT',\n",
       " 'ZRXUSDT',\n",
       " 'COSUSDT',\n",
       " 'ALGOUSDT',\n",
       " 'OMGUSDT',\n",
       " 'WAVESUSDT',\n",
       " 'GTOUSDT',\n",
       " 'ONEUSDT',\n",
       " 'DASHUSDT',\n",
       " 'LTCUSDT',\n",
       " 'LINKUSDT',\n",
       " 'NANOUSDT',\n",
       " 'VETUSDT',\n",
       " 'DOGEUSDT',\n",
       " 'NULSUSDT',\n",
       " 'FETUSDT',\n",
       " 'IOTAUSDT',\n",
       " 'BNBUSDT',\n",
       " 'QTUMUSDT',\n",
       " 'KEYUSDT',\n",
       " 'TOMOUSDT',\n",
       " 'ONGUSDT',\n",
       " 'ICXUSDT',\n",
       " 'ETHUSDT',\n",
       " 'MATICUSDT',\n",
       " 'TRXUSDT',\n",
       " 'ZECUSDT',\n",
       " 'TFUELUSDT',\n",
       " 'ADAUSDT',\n",
       " 'ZILUSDT',\n",
       " 'XLMUSDT',\n",
       " 'ENJUSDT',\n",
       " 'FTMUSDT',\n",
       " 'CELRUSDT',\n",
       " 'MTLUSDT',\n",
       " 'BATUSDT',\n",
       " 'NEOUSDT',\n",
       " 'MITHUSDT',\n",
       " 'XRPUSDT',\n",
       " 'DUSKUSDT',\n",
       " 'STRAXBUSD',\n",
       " 'SYSBUSD',\n",
       " 'WINGBUSD',\n",
       " 'PONDBUSD',\n",
       " 'AXSBUSD',\n",
       " 'EGLDBUSD',\n",
       " 'AUDIOBUSD',\n",
       " 'SKLBUSD',\n",
       " 'BAKEBUSD',\n",
       " 'YFIIBUSD',\n",
       " 'TRBBUSD',\n",
       " 'CHZBUSD',\n",
       " 'RSRBUSD',\n",
       " 'SFPBUSD',\n",
       " 'DEGOBUSD',\n",
       " 'CTKBUSD',\n",
       " 'FLMBUSD',\n",
       " 'OMBUSD',\n",
       " 'BELBUSD',\n",
       " 'ROSEBUSD',\n",
       " '1INCHBUSD',\n",
       " 'ONEBUSD',\n",
       " 'XVSBUSD',\n",
       " 'UNFIBUSD',\n",
       " 'DIABUSD',\n",
       " 'AERGOBUSD',\n",
       " 'CFXBUSD',\n",
       " 'BTGBUSD',\n",
       " 'ALPHABUSD',\n",
       " 'KSMBUSD',\n",
       " 'DOTBUSD',\n",
       " 'TLMBUSD',\n",
       " 'TKOBUSD',\n",
       " 'AVAXBUSD',\n",
       " 'OMGBUSD',\n",
       " 'FILBUSD',\n",
       " 'INJBUSD',\n",
       " 'NMRBUSD',\n",
       " 'NEARBUSD',\n",
       " 'DOGEBUSD',\n",
       " 'SRMBUSD',\n",
       " 'IOTABUSD',\n",
       " 'BALBUSD',\n",
       " 'AUDBUSD',\n",
       " 'AVABUSD',\n",
       " 'CRVBUSD',\n",
       " 'YFIBUSD',\n",
       " 'OCEANBUSD',\n",
       " 'LINABUSD',\n",
       " 'LUNABUSD',\n",
       " 'BURGERBUSD',\n",
       " 'UNIBUSD',\n",
       " 'FRONTBUSD',\n",
       " 'SHIBBUSD',\n",
       " 'DNTBUSD',\n",
       " 'JSTBUSD',\n",
       " 'CAKEBUSD',\n",
       " 'ICPBUSD',\n",
       " 'ANTBUSD',\n",
       " 'ARBUSD',\n",
       " 'ALICEBUSD',\n",
       " 'GRTBUSD',\n",
       " 'ZRXBUSD',\n",
       " 'DODOBUSD',\n",
       " 'AAVEBUSD',\n",
       " 'DEXEBUSD',\n",
       " 'SUPERBUSD',\n",
       " 'ZILBUSD',\n",
       " 'MATICBUSD',\n",
       " 'KNCBUSD',\n",
       " 'CTSIBUSD',\n",
       " 'SUSHIBUSD',\n",
       " 'GBPBUSD',\n",
       " 'LRCBUSD',\n",
       " 'SANDBUSD',\n",
       " 'BZRXBUSD',\n",
       " 'SXPBUSD',\n",
       " 'RUNEBUSD',\n",
       " 'HBARBUSD',\n",
       " 'DGBBUSD',\n",
       " 'WRXBUSD',\n",
       " 'MANABUSD',\n",
       " 'SNXBUSD',\n",
       " 'MKRBUSD',\n",
       " 'COMPBUSD',\n",
       " 'AKROUSDT',\n",
       " 'FIROUSDT',\n",
       " 'VTHOUSDT',\n",
       " 'PUNDIXUSDT',\n",
       " 'XEMUSDT',\n",
       " 'LINAUSDT',\n",
       " 'CKBUSDT',\n",
       " 'REEFUSDT',\n",
       " '1INCHUSDT',\n",
       " 'ALICEUSDT',\n",
       " 'ROSEUSDT',\n",
       " 'AVAXUSDT',\n",
       " 'CELOUSDT',\n",
       " 'BLZUSDT',\n",
       " 'BTTBUSD',\n",
       " 'DATABUSD',\n",
       " 'ONTBUSD',\n",
       " 'ALGOBUSD',\n",
       " 'RVNBUSD',\n",
       " 'ZECBUSD',\n",
       " 'TOMOBUSD',\n",
       " 'SOLBUSD',\n",
       " 'XMRBUSD',\n",
       " 'DATAUSDT',\n",
       " 'OCEANUSDT',\n",
       " 'CHRUSDT',\n",
       " 'BELUSDT',\n",
       " 'TKOUSDT',\n",
       " 'ARDRUSDT',\n",
       " 'YFIUSDT',\n",
       " 'SKLUSDT',\n",
       " 'ALPHAUSDT',\n",
       " 'MANAUSDT',\n",
       " 'GRTUSDT',\n",
       " 'MBLUSDT',\n",
       " 'UTKUSDT',\n",
       " 'LITUSDT',\n",
       " 'PAXGUSDT',\n",
       " 'GBPUSDT',\n",
       " 'UNFIUSDT',\n",
       " 'SUNUSDT',\n",
       " 'FILUSDT',\n",
       " 'ANTUSDT',\n",
       " 'BALUSDT',\n",
       " 'SFPUSDT',\n",
       " 'OXTUSDT',\n",
       " 'PONDUSDT',\n",
       " 'CTSIUSDT',\n",
       " 'RUNEUSDT',\n",
       " 'KNCUSDT',\n",
       " 'LRCUSDT',\n",
       " 'WTCUSDT',\n",
       " 'DEGOUSDT',\n",
       " 'AAVEUSDT',\n",
       " 'BTCUSDT',\n",
       " 'IOSTUSDT',\n",
       " 'SUPERUSDT',\n",
       " 'STMXUSDT',\n",
       " 'NBSUSDT',\n",
       " 'SRMUSDT',\n",
       " 'DOTUSDT',\n",
       " 'AUDIOUSDT',\n",
       " 'CAKEUSDT',\n",
       " 'SOLUSDT',\n",
       " 'SUSHIUSDT',\n",
       " 'DCRUSDT',\n",
       " 'WINGUSDT',\n",
       " 'CFXUSDT',\n",
       " 'DODOUSDT',\n",
       " 'BAKEUSDT',\n",
       " 'BZRXUSDT',\n",
       " 'BTGUSDT',\n",
       " 'HARDUSDT',\n",
       " 'AXSUSDT',\n",
       " 'FIOUSDT',\n",
       " 'STPTUSDT',\n",
       " 'UNIUSDT']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(results_df[\"Symbol\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results_df[results_df[\"Symbol\"] == \"XLMUSDT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(train_data, num_models=10, verbose=False, row_subsample=False, max_iterations=10, bagging=True, bagging_split=0.9):\n",
    "    log_models = 0\n",
    "    bt_models = 0\n",
    "    rf_models = 0\n",
    "    models = []\n",
    "    \n",
    "    def get_sub_train_data():\n",
    "        if bagging:\n",
    "            sub_train_data, _ = train_data.random_split(bagging_split)\n",
    "        else:\n",
    "            sub_train_data = train_data\n",
    "        return sub_train_data\n",
    "    \n",
    "    while log_models < num_models:\n",
    "        models.append(tc.logistic_classifier.create(get_sub_train_data(), target=label_column[0], class_weights = 'auto', verbose=verbose))\n",
    "        log_models += 1\n",
    "    \n",
    "    while bt_models < num_models:\n",
    "        models.append(tc.boosted_trees_classifier.create(get_sub_train_data(), target=label_column[0], column_subsample=True, max_iterations=max_iterations, class_weights = 'auto', verbose=verbose))\n",
    "        bt_models += 1\n",
    "    \n",
    "    while rf_models < num_models:\n",
    "        models.append(tc.random_forest_classifier.create(get_sub_train_data(), target=label_column[0], column_subsample=True, max_iterations=max_iterations, class_weights = 'auto', verbose=verbose))\n",
    "        rf_models += 1\n",
    "    return models\n",
    "    \n",
    "def train_svm_models(train_data, num_models=10, verbose=False, max_iterations=13):\n",
    "    svm_models = 0\n",
    "    models = []\n",
    "    while svm_models < num_models:\n",
    "        models.append(tc.svm_classifier.create(train_data, target=label_column[0], max_iterations=max_iterations, class_weights = 'auto', verbose=verbose))\n",
    "        svm_models += 1\n",
    "    return models\n",
    "        \n",
    "    \n",
    "def train_ensemble_on_symbols(symbols, data, split_perc=0.8, num_models=10, verbose=False, row_subsample=False):\n",
    "    \"\"\"\n",
    "    Trains only on a subset of symbols\n",
    "    \"\"\"\n",
    "    if not len(symbols):\n",
    "        raise Exception(f\"Symbols array cannot be empty\")\n",
    "    \n",
    "    filtered_data = data.filter_by(symbols, 'Symbol')\n",
    "    filtered_data = apply_transformations(filtered_data)\n",
    "    train_data, test_data = filtered_data.random_split(split_perc)\n",
    "    models = train_ensemble(train_data, num_models=num_models, verbose=verbose, row_subsample=row_subsample)\n",
    "    return models, train_data, test_data\n",
    "        \n",
    "def get_probs(models, test_data):\n",
    "    probs = []\n",
    "    for model in models:\n",
    "        probs.append(model.predict(test_data, output_type = 'probability'))\n",
    "    return probs\n",
    "\n",
    "def average_probabilities(probabilities):\n",
    "\n",
    "    averaged_probs = []\n",
    "    for i in range(len(probabilities[0])):\n",
    "        averaged_probs.append(sum([pred_prob[i] for pred_prob in probabilities]) / len(probabilities))\n",
    "    return averaged_probs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(averaged_probs, test_data, threshold=0.6):\n",
    "    threshold = threshold\n",
    "    labels = {\n",
    "        \"False/False\": 0,\n",
    "        \"False/True\": 0,\n",
    "        \"True/True\": 0,\n",
    "        \"True/False\": 0\n",
    "    }\n",
    "    results = {\n",
    "        \"precision\": None,\n",
    "        \"recall\": None,\n",
    "        \"specificity\": None,\n",
    "        \"auc\": None,\n",
    "        \"f1_score\": None,\n",
    "        \"num_targets\": None,\n",
    "        \"labels\": None\n",
    "    }\n",
    "    def label_is_true(index) -> bool:\n",
    "        if test_data[\"Profitable\"][index] == \"True\" or test_data[\"Profitable\"][index] == 1:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    target_labels = list(test_data[\"Profitable\"])\n",
    "    for i in range(len(averaged_probs)):\n",
    "        prob = averaged_probs[i]\n",
    "        target_label = target_labels[i]\n",
    "\n",
    "        if prob > threshold:\n",
    "            if label_is_true(i):\n",
    "                labels[\"True/True\"] += 1\n",
    "            else:\n",
    "                labels[\"False/True\"] += 1\n",
    "        else:\n",
    "            if label_is_true(i):\n",
    "                labels[\"True/False\"] += 1\n",
    "            else:\n",
    "                labels[\"False/False\"] += 1\n",
    "    \n",
    "    \n",
    "    total_predicted_true = labels[\"True/True\"] + labels[\"False/True\"]\n",
    "    total_actually_true = (labels[\"True/True\"] + labels[\"True/False\"])\n",
    "\n",
    "    results[\"precision\"] = 0 if total_predicted_true == 0 else labels[\"True/True\"] / (labels[\"True/True\"] + labels[\"False/True\"])\n",
    "    results[\"recall\"] = 0 if total_actually_true == 0 else labels[\"True/True\"] / (labels[\"True/True\"] + labels[\"True/False\"])\n",
    "    \n",
    "    if all(list(target_labels)):\n",
    "        auc_score = 1.0\n",
    "    elif not any(list(target_labels)):\n",
    "        auc_score = 0.0\n",
    "    else: \n",
    "        auc_score = roc_auc_score(target_labels, averaged_probs)\n",
    "        \n",
    "    results[\"auc\"] = auc_score\n",
    "    results[\"f1_score\"] = 0 if not results[\"precision\"] and not results[\"recall\"] else 2 * ((results[\"precision\"] * results[\"recall\"]) / (results[\"precision\"] + results[\"recall\"]))\n",
    "    results[\"specificity\"] = labels[\"False/False\"] / (labels[\"False/False\"] + labels[\"False/True\"])\n",
    "    results[\"num_targets\"] = len(target_labels)\n",
    "    results[\"labels\"] = labels\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_generalization(data, num_models=10, row_subsample=False, verbose=False, split_perc = 0.8):\n",
    "    \"\"\"\n",
    "    Holds out symbols and evaluates ensemble performance on the holdout symbol.\n",
    "    \n",
    "    \"\"\"\n",
    "    symbols = data[\"Symbol\"].unique()\n",
    "    results = dict(zip(symbols, [None]*len(symbols)))\n",
    "    random.seed()\n",
    "    seed = random.random()\n",
    "    for i, symbol in enumerate(symbols):\n",
    "        test_data = data[data[\"Symbol\"] == symbol]\n",
    "        sf = data[data[\"Symbol\"] != symbol]\n",
    "        assert(len(test_data) > 0)\n",
    "        \n",
    "        train_data, _ = sf.random_split(split_perc)#, seed=seed)\n",
    "        assert(len(train_data[train_data[\"Symbol\"] == symbol]) == 0)\n",
    "        \n",
    "        test_data = apply_transformations(test_data)\n",
    "        train_data = apply_transformations(train_data)\n",
    "        \n",
    "        models = train_ensemble(train_data, num_models=num_models, row_subsample=row_subsample, verbose=verbose)\n",
    "        probs = get_probs(models, test_data)\n",
    "        averaged_probs = average_probabilities(probs)\n",
    "        eval_results = evaluate_model(averaged_probs, test_data)\n",
    "        results[symbol] = {\n",
    "            \"precision\": eval_results[\"precision\"],\n",
    "            \"recall\": eval_results[\"recall\"],\n",
    "            \"auc\": eval_results[\"auc\"],\n",
    "            \"f1_score\": eval_results[\"f1_score\"],\n",
    "            \"specificity\": eval_results[\"specificity\"],\n",
    "            \"num_targets\": eval_results[\"num_targets\"],\n",
    "            \"real_success_rate\": len(test_data[test_data[\"Profitable\"] == 1]) / len(test_data)\n",
    "        }\n",
    "        print(f\"[{i + 1}/{len(symbols)}] Evaluated {symbol}, Results: {results[symbol]}\")\n",
    "    return results\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_symbols(generalization_results, min_precision = 0.62, min_baseline_perc = 0.65, min_model_improvement_perc=0.15, min_recall=0.1, verbose=False):\n",
    "    best_symbols = []\n",
    "    for symbol in generalization_results.keys():\n",
    "        results = generalization_results[symbol]\n",
    "        precision = results[\"precision\"]\n",
    "        recall = results[\"recall\"]\n",
    "        real_success_rate = results[\"real_success_rate\"]\n",
    "        model_improvement_perc = 0.0\n",
    "        \n",
    "        \n",
    "        # Skip any symbols where the model performs worse than real life\n",
    "        if precision <= real_success_rate:\n",
    "            if verbose:\n",
    "                print(f\"[{symbol}] Precision less than success rate, skipping\")\n",
    "            continue\n",
    "        else:\n",
    "            model_improvement_perc = (precision - real_success_rate) / real_success_rate\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"[{symbol}] <> Precision: {round(precision,2)} <> Recall: {round(recall,2)} <> Orig Accuracy: {round(real_success_rate * 100.0, 2)}% <> Baseline Improvement: {round(model_improvement_perc * 100.0, 2)}%\")\n",
    "        \n",
    "        if real_success_rate >= min_baseline_perc and precision >= min_baseline_perc:\n",
    "            pass\n",
    "        else:\n",
    "            if precision < min_precision:\n",
    "                continue\n",
    "            if recall < min_recall:\n",
    "                continue\n",
    "            if model_improvement_perc < min_model_improvement_perc:\n",
    "                continue\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Selected [{symbol}]\")\n",
    "        best_symbols.append(symbol)\n",
    "    return best_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(results_df)\n",
    "results = evaluate_generalization(data, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_symbols = select_best_symbols(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models, train_data, test_data = train_ensemble_on_symbols(best_symbols, data, row_subsample=True)\n",
    "probs = get_probs(models, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_probs = average_probabilities(probs)\n",
    "model_results = evaluate_model(averaged_probs, test_data, threshold=0.65)\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation_pipeline(results_df, generalization_cycles = 3, min_precision = 0.62, min_model_improvement_perc=0.15, min_recall=0.1, split_perc=0.8, num_models=10, row_subsample=False, threshold=0.6, verbose=False):\n",
    "    data = get_data(results_df)\n",
    "    all_generalization_results = []\n",
    "    averaged_generalization_results = {}\n",
    "    for _ in range(generalization_cycles):\n",
    "        generalization_results = evaluate_generalization(data, num_models=num_models, row_subsample=row_subsample, verbose=verbose)\n",
    "        all_generalization_results.append(generalization_results)\n",
    "    \n",
    "    \n",
    "    for symbol in all_generalization_results[0].keys():\n",
    "        if symbol not in averaged_generalization_results.keys():\n",
    "                averaged_generalization_results[symbol] = {}\n",
    "        \n",
    "        sum_precision, sum_recall, sum_specificity, sum_real_success_rate, sum_auc, sum_f1_score, sum_num_targets = 0, 0, 0, 0, 0, 0, 0\n",
    "        for gen_result in all_generalization_results:\n",
    "                sum_precision += gen_result[symbol][\"precision\"]\n",
    "                sum_recall += gen_result[symbol][\"recall\"]\n",
    "                sum_specificity += gen_result[symbol][\"specificity\"]\n",
    "                sum_real_success_rate += gen_result[symbol][\"real_success_rate\"]\n",
    "                sum_auc += gen_result[symbol][\"auc\"]\n",
    "                sum_f1_score += gen_result[symbol][\"f1_score\"]\n",
    "                sum_num_targets += gen_result[symbol][\"num_targets\"]\n",
    "        \n",
    "        averaged_generalization_results[symbol][\"precision\"] = sum_precision / len(all_generalization_results)\n",
    "        averaged_generalization_results[symbol][\"recall\"] = sum_recall / len(all_generalization_results)\n",
    "        averaged_generalization_results[symbol][\"specificity\"] = sum_specificity / len(all_generalization_results)\n",
    "        averaged_generalization_results[symbol][\"real_success_rate\"] = sum_real_success_rate / len(all_generalization_results)\n",
    "        averaged_generalization_results[symbol][\"auc\"] = sum_auc / len(all_generalization_results)\n",
    "        averaged_generalization_results[symbol][\"f1_score\"] = sum_f1_score / len(all_generalization_results)\n",
    "        averaged_generalization_results[symbol][\"num_targets\"] = sum_num_targets / len(all_generalization_results)\n",
    "            \n",
    "    best_symbols = select_best_symbols(averaged_generalization_results, min_precision=min_precision, min_model_improvement_perc=min_model_improvement_perc, min_recall=min_recall, verbose=verbose)\n",
    "    models_trained_on_best_symbols, train_data, test_data = train_ensemble_on_symbols(best_symbols, data, split_perc=split_perc, num_models=num_models, row_subsample=row_subsample, verbose=verbose)\n",
    "    \n",
    "    probs = get_probs(models_trained_on_best_symbols, test_data)\n",
    "    averaged_probs = average_probabilities(probs)\n",
    "    model_results = evaluate_model(averaged_probs, test_data, threshold=threshold)\n",
    "    return models_trained_on_best_symbols, best_symbols, model_results, train_data, test_data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /root/trading-libs/symphony/backtest/results/demark/demark.csv.gz</pre>"
      ],
      "text/plain": [
       "Finished parsing file /root/trading-libs/symphony/backtest/results/demark/demark.csv.gz"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.139252 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.139252 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[float,float,float,float,float,str,int,str,str,str,str,int,int,float,float,str,str,str,float,float,float,float,float,str,float,str,str,float,str,str,str,str,str,float,float,int,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 3830 lines. Lines per second: 30780.1</pre>"
      ],
      "text/plain": [
       "Read 3830 lines. Lines per second: 30780.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /root/trading-libs/symphony/backtest/results/demark/demark.csv.gz</pre>"
      ],
      "text/plain": [
       "Finished parsing file /root/trading-libs/symphony/backtest/results/demark/demark.csv.gz"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 9600 lines in 0.154259 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 9600 lines in 0.154259 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/146] Evaluated ADAUSDT, Results: {'precision': 0.625, 'recall': 0.2777777777777778, 'auc': 0.6458333333333333, 'f1_score': 0.3846153846153846, 'specificity': 0.85, 'num_targets': 76, 'real_success_rate': 0.47368421052631576}\n",
      "[2/146] Evaluated SANDBUSD, Results: {'precision': 0.8461538461538461, 'recall': 0.5238095238095238, 'auc': 0.7651888341543513, 'f1_score': 0.6470588235294118, 'specificity': 0.9310344827586207, 'num_targets': 50, 'real_success_rate': 0.42}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-9f8da6733b45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodels_trained_on_best_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_evaluation_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-42f007ebae4c>\u001b[0m in \u001b[0;36mrun_evaluation_pipeline\u001b[0;34m(results_df, generalization_cycles, min_precision, min_model_improvement_perc, min_recall, split_perc, num_models, row_subsample, threshold, verbose)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0maveraged_generalization_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneralization_cycles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mgeneralization_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_generalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_models\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_subsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrow_subsample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mall_generalization_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneralization_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-208939015d47>\u001b[0m in \u001b[0;36mevaluate_generalization\u001b[0;34m(data, num_models, row_subsample, verbose, split_perc)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_transformations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ensemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_models\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_subsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrow_subsample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0maveraged_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-52103c3d8064>\u001b[0m in \u001b[0;36mtrain_ensemble\u001b[0;34m(train_data, num_models, verbose, row_subsample, max_iterations, bagging, bagging_split)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mrf_models\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnum_models\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_forest_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_sub_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_column\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_subsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mrf_models\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/turicreate/toolkits/classifier/random_forest_classifier.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(dataset, target, features, max_iterations, validation_set, verbose, class_weights, random_seed, metric, **kwargs)\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     )\n\u001b[1;32m    629\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__proxy__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/turicreate/toolkits/_supervised_learning.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(dataset, target, model_name, features, validation_set, distributed, verbose, seed, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_turicreate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mQuietProgress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mSupervisedLearningModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/turicreate/extensions.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;31m# is it a function?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__run_class_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m             ret.__doc__ = (\n\u001b[1;32m    307\u001b[0m                 \u001b[0;34m\"Name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\nParameters: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/turicreate/extensions.py\u001b[0m in \u001b[0;36m__run_class_function\u001b[0;34m(self, fnname, args, kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;31m# unwrap it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tkclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfnname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margument_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;31m# Expose C++ exceptions using ToolkitError.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models_trained_on_best_symbols, best_symbols, model_results, train_data, test_data = run_evaluation_pipeline(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_symbols, model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(get_probs(models_trained_on_best_symbols, test_data), test_data, threshold=0.625)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_models = train_svm_models(train_data, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pred_sframe_row(models, row, label_column = \"Profitable\", svm_models=[]):\n",
    "    sf_row = {}\n",
    "    max_digits = len(str(len(models)))\n",
    "    for i, model in enumerate(models + svm_models):\n",
    "        model_formatted_num = format(i, f'0{max_digits}d')\n",
    "        key = \"Model\" + model_formatted_num\n",
    "        if type(model) != tc.toolkits.classifier.svm_classifier.SVMClassifier:\n",
    "            sf_row[key] = [model.predict(row, output_type='probability')[0]]\n",
    "        else:\n",
    "            pred = model.predict(row)[0]\n",
    "            pred = 1 if pred == 'True' or pred == 1 else 0\n",
    "            sf_row[key] = [pred]\n",
    "    sf_row[label_column] = [row[label_column]]\n",
    "    return tc.SFrame(sf_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meta_model = tc.logistic_classifier.create(train_sf, target=label_column[0], max_iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_meta_model_sframe(models, target_sf, svm_models=[]):\n",
    "    new_sf = tc.SFrame()\n",
    "    for row in target_sf:\n",
    "        sf_row = create_pred_sframe_row(models, row, svm_models=svm_models)\n",
    "        new_sf = new_sf.append(sf_row)\n",
    "    return new_sf\n",
    "        \n",
    "        \n",
    "\n",
    "def meta_model_predict(models, meta_model, target_sf):\n",
    "    #meta_model_sframe = create_meta_model_sframe(models, target_sf)\n",
    "    def predict(index):\n",
    "        return meta_model.predict(target_sf[index], output_type='probability')[0]\n",
    "    return list(\n",
    "        map(\n",
    "            predict, [i for i in range(len(target_sf))])\n",
    "           )\n",
    "    #return [meta_model.predict(row, output_type='probability')[0] for row in meta_model_sframe]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probabilities = meta_model_predict(models_trained_on_best_symbols, meta_model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meta_model_train_data = create_meta_model_sframe(models_trained_on_best_symbols, train_data, svm_models=svm_models)\n",
    "#meta_model_test_data = create_meta_model_sframe(models_trained_on_best_symbols, test_data, svm_models=svm_models)\n",
    "meta_model_train_data = create_meta_model_sframe(models_trained_on_best_symbols, train_data)\n",
    "meta_model_test_data = create_meta_model_sframe(models_trained_on_best_symbols, test_data)\n",
    "#meta_model_train_data[\"Profitable\"] = meta_model_train_data[\"Profitable\"].apply(lambda l: 1 if l == 'True' else 0).to_numpy()\n",
    "#meta_model_test_data[\"Profitable\"] = meta_model_test_data[\"Profitable\"].apply(lambda l: 1 if l == 'True' else 0).to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meta_model = tc.logistic_classifier.create(meta_model_train_data, target=label_column[0], max_iterations=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probabilities = meta_model_predict(models_trained_on_best_symbols, meta_model, meta_model_test_data)\n",
    "#results = evaluate_model([probabilities], meta_model_test_data, threshold=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaDataset(Dataset):\n",
    "    def __init__(self, sf: tc.SFrame, label_column: str = \"Profitable\"):\n",
    "        self.df = sf.to_dataframe()\n",
    "        self.labels = torch.from_numpy(\n",
    "            self.df[label_column].to_numpy()\n",
    "        )\n",
    "        self.X = torch.from_numpy(\n",
    "            self.df.drop(columns = [label_column], axis = 1).to_numpy()\n",
    "        )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        ensemble_outputs = self.X[idx]\n",
    "        return [ensemble_outputs.float(), label]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "md = MetaDataset(meta_model_train_data)\n",
    "class_counts = [\n",
    "    len(meta_model_train_data[meta_model_train_data[\"Profitable\"] == 1]),\n",
    "    len(meta_model_train_data[meta_model_train_data[\"Profitable\"] == 0])\n",
    "]\n",
    "num_samples = sum(class_counts)\n",
    "labels = \\\n",
    "    list(meta_model_train_data[meta_model_train_data[\"Profitable\"] == 1][\"Profitable\"]) + \\\n",
    "    list(meta_model_train_data[meta_model_train_data[\"Profitable\"] == 0][\"Profitable\"])\n",
    "class_weights = [num_samples/class_counts[i] for i in range(len(class_counts))]\n",
    "weights = [class_weights[labels[i]] for i in range(int(num_samples))]\n",
    "sampler = WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))\n",
    "\n",
    "#loader = DataLoader(md, batch_size=BATCH_SIZE, shuffle=False, sampler=sampler)\n",
    "loader = DataLoader(md, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, opt, criterion, loader, batch_size=50):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for x_batch, y_batch in loader:\n",
    "        x_batch = Variable(x_batch)\n",
    "        y_batch = Variable(y_batch)\n",
    "        \n",
    "        y_batch = y_batch.float()\n",
    "        opt.zero_grad()\n",
    "        # (1) Forward\n",
    "        y_hat = model(x_batch)\n",
    "        # (2) Compute diff\n",
    "        loss = criterion(y_hat, y_batch.unsqueeze(1))\n",
    "        # (3) Compute gradients\n",
    "        loss.backward()\n",
    "        # (4) update weights\n",
    "        opt.step()        \n",
    "        losses.append(loss.data.numpy())\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNBinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_features = 30, batch_size = 64):\n",
    "        super(DNNBinaryClassifier, self).__init__()\n",
    "        \n",
    "        hidden_nodes1 = 64\n",
    "        hidden_nodes2 = 32\n",
    "        self.fc1 = nn.Linear(input_features, batch_size)\n",
    "        self.relu1 = nn.LeakyReLU()\n",
    "        self.bn1d1 = nn.BatchNorm1d(batch_size, input_features)\n",
    "        self.dout1 = nn.Dropout(0.25)\n",
    "        self.fc2 = nn.Linear(batch_size, hidden_nodes2)\n",
    "        self.relu2 = nn.LeakyReLU()\n",
    "        self.dout2 = nn.Dropout(0.25)\n",
    "        self.prelu = nn.PReLU(1)\n",
    "        self.out = nn.Linear(hidden_nodes2, 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #self.dout2 = nn.Dropout(0.3)\n",
    "        #self.bn1d2 = nn.BatchNorm1d(hidden_nodes2, batch_size)\n",
    "        #self.fc3 = nn.Linear(hidden_nodes1, hidden_nodes2)\n",
    "        #self.out_act = nn.Softmax()\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        #x = self.bn1d(inputs)\n",
    "        #x = self.fc1(x)\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.relu1(x)\n",
    "        x = self.bn1d1(x)\n",
    "        x = self.dout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dout2(x)\n",
    "        x = self.prelu(x)\n",
    "        x = self.out(x)\n",
    "        \n",
    "        #x = self.bn1d2(x)\n",
    "        #x = self.dout2(x)\n",
    "        #x = self.fc3(x)\n",
    "        #x = self.prelu(x)\n",
    "        #x = self.out_act(x)\n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_features = len(models_trained_on_best_symbols)# + len(svm_models)\n",
    "model = DNNBinaryClassifier(input_features=input_features )\n",
    "model.to(device)\n",
    "EPOCHS = 20\n",
    "\n",
    "LEARNING_RATE = 3e-4\n",
    "e_losses = []\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=5e-3)\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    e_losses += train_epoch(model, optimizer, criterion, loader, batch_size=BATCH_SIZE)\n",
    "plt.plot(e_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test = torch.from_numpy(meta_model_train_data.to_numpy()[10][:-1].astype(float)).float()\n",
    "float(torch.sigmoid(model(test.unsqueeze(0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_for_nn_meta_model(data, meta_model, verbose=False):\n",
    "    meta_model.eval()\n",
    "    \n",
    "    def make_prediction(index):\n",
    "        tensor = torch.from_numpy(data.to_numpy()[index][:-1].astype(float)).float()\n",
    "        pred = float(torch.sigmoid(meta_model(tensor.unsqueeze(0))))\n",
    "        label = train_data[index][\"Profitable\"]\n",
    "        if verbose:\n",
    "            print(f\"Label {'True' if label else 'False'}, Confidence: {pred}\")\n",
    "        return pred\n",
    "    \n",
    "    predictions = list(map(make_prediction, [i for i in range(len(data))]))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = predictions_for_nn_meta_model(meta_model_train_data, model, verbose=True)\n",
    "train_results = evaluate_model([train_predictions], meta_model_train_data)\n",
    "train_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = predictions_for_nn_meta_model(meta_model_test_data, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = evaluate_model([test_predictions], meta_model_test_data, threshold=0.9)\n",
    "test_results\n",
    "real_success_rate = len(meta_model_test_data[meta_model_test_data[\"Profitable\"] == 1]) / len(meta_model_test_data)\n",
    "test_results, real_success_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
